From 13a7e645095fb2ae7a1b620222bb07bdacb1a359 Mon Sep 17 00:00:00 2001
From: Shawn Zhong <github@shawnzhong.com>
Date: Thu, 20 Feb 2020 02:30:29 -0600
Subject: [PATCH] minor

---
 mimick/inter_nearest_vecs.py   | 25 ++++++++++++-------------
 mimick/model.py                |  4 +---
 mimick/util.py                 |  7 +++++--
 scripts/output_word_vectors.py |  5 ++++-
 utils.py                       |  2 +-
 5 files changed, 23 insertions(+), 20 deletions(-)

diff --git a/mimick/inter_nearest_vecs.py b/mimick/inter_nearest_vecs.py
index f256be3..686ff5b 100755
--- a/mimick/inter_nearest_vecs.py
+++ b/mimick/inter_nearest_vecs.py
@@ -39,19 +39,22 @@ if __name__ == "__main__":
     parser.add_argument("--window-width", type=int, default=DEFAULT_WINDOW_WIDTH, help="Width of CNN layers (default = 3)")
     parser.add_argument("--pooling-maxk", type=int, default=DEFAULT_POOLING_MAXK, help="K for K-max pooling (default = 1)")
     parser.add_argument("--stride", dest="w_stride", default=DEFAULT_STRIDE, help="'Width' stride for CNN layers (default = 1)")
-    
+
+    parser.add_argument("--output", required=True, dest="output", help="Output filename (.txt)")
+    parser.add_argument("--vocab", dest="vocab", help="File containing words for test set (optional)")
+
     opts = parser.parse_args()
 
     # load vocab
     if opts.w2v_format:
         voc_words, voc_vecs = read_text_embs(opts.vectors)
     else:
-        voc_words, voc_vecs = pickle.load(open(opts.vectors))
+        voc_words, voc_vecs = pickle.load(open(opts.vectors, "rb"), encoding='bytes')
 
     we_dim = len(voc_vecs[0])
     
     # load model
-    c2i = pickle.load(open(opts.c2i))
+    c2i = pickle.load(open(opts.c2i, "rb"))
     if opts.use_cnn:
         mimick = CNNMimick(c2i, num_conv_layers=opts.num_conv_layers, char_dim=opts.char_dim,\
                            hidden_dim=opts.hidden_dim, window_width=opts.window_width,\
@@ -62,14 +65,10 @@ if __name__ == "__main__":
                             hidden_dim=opts.hidden_dim,\
                             word_embedding_dim=we_dim, file=opts.mimick)
 
-    # prompt
-    while True:
-        sys.stdout.write('> ')
-        next_word = sys.stdin.readline().strip()
-        if next_word in QUITTING_WORDS:
-            exit()
+    with open(opts.vocab) as fin, open(opts.output, 'w+') as fout:
+        for line in fin:
+            next_word = line.strip()
 
-        word_chars = [c2i[c] for c in next_word]
-        pred_vec = mimick.predict_emb(word_chars).value()
-        top_k = sorted([(iv, dist(iv_vec, pred_vec)) for iv,iv_vec in zip(voc_words, voc_vecs)], key=lambda x: x[1])[:opts.ktop]
-        print('\n'.join(['{}:\t{:.3f}'.format(near[0], 1.0 - near[1]) for near in top_k]))
+            word_chars = [c2i[c] for c in next_word]
+            pred_vec = mimick.predict_emb(word_chars).value()
+            print(next_word, *pred_vec, file=fout, sep=" ")
diff --git a/mimick/model.py b/mimick/model.py
index 64c0972..5b7d156 100644
--- a/mimick/model.py
+++ b/mimick/model.py
@@ -120,7 +120,6 @@ class CNNMimick:
         # character mapping saved separately
         pickle.dump(self.c2i, open(file_name[:-4] + '.c2i', 'w'))
 
-    @property
     def model(self):
         return self.model
 
@@ -215,7 +214,6 @@ class LSTMMimick:
         self.mlp_out = next(model_members)
         self.mlp_out_bias = next(model_members)
 
-    @property
     def model(self):
         return self.model
 
@@ -300,7 +298,7 @@ if __name__ == "__main__":
     root_logger.info("Hidden dimension: {}".format(options.hidden_dim))
 
     # Load training set
-    dataset = pickle.load(open(options.dataset, "rb"))
+    dataset = pickle.load(open(options.dataset, "rb"), encoding='bytes')
     c2i = dataset["c2i"]
     i2c = { i: c for c, i in list(c2i.items()) } # inverse map
     training_instances = dataset["training_instances"]
diff --git a/mimick/util.py b/mimick/util.py
index 1ac947e..ce66647 100644
--- a/mimick/util.py
+++ b/mimick/util.py
@@ -1,3 +1,6 @@
+import pickle
+
+
 def wordify(instance, i2c):
     return ''.join([i2c[i] for i in instance.chars])
 
@@ -15,7 +18,7 @@ def read_text_embs(files):
     '''
     word_embs = dict()
     for filename in files:
-        with codecs.open(filename, "r", "utf-8") as f:
+        with open(filename, "r") as f:
             for line in f:
                 split = line.split()
                 if len(split) > 2:
@@ -29,6 +32,6 @@ def read_pickle_embs(files):
     word_embs = dict()
     for filename in files:
         print(filename)
-        words, embs = pickle.load(open(filename, "r"))
+        words, embs = pickle.load(open(filename, "rb"), encoding='bytes')
         word_embs.update(list(zip(words, embs)))
     return list(zip(*iter(word_embs.items())))
diff --git a/scripts/output_word_vectors.py b/scripts/output_word_vectors.py
index c1b56c3..0030150 100755
--- a/scripts/output_word_vectors.py
+++ b/scripts/output_word_vectors.py
@@ -7,12 +7,15 @@ import pickle
 import argparse
 import codecs
 import numpy as np
+import collections
 
 __author__ = "Yuval Pinter and Robert Guthrie, 2017"
 
 POLYGLOT_UNK = str("<UNK>")
 W2V_UNK = str("UNK")
 
+Instance = collections.namedtuple("Instance", ["chars", "word_emb"])
+
 def read_text_embs(files):
     word_embs = dict()
     for filename in files:
@@ -28,7 +31,7 @@ def read_pickle_embs(files):
     for filename in files:
         print(filename)
         #this might need to be a "rb" read mode
-        words, embs = pickle.load(open(filename, "r"))
+        words, embs = pickle.load(open(filename, "rb"))
         word_embs.update(list(zip(words, embs)))
     return list(zip(*word_embs.items()))
 
diff --git a/utils.py b/utils.py
index bd6b895..f889cc5 100644
--- a/utils.py
+++ b/utils.py
@@ -23,7 +23,7 @@ class CSVLogger:
 
 def read_pretrained_embeddings(filename, w2i):
     word_to_embed = {}
-    with codecs.open(filename, "r", "utf-8") as f:
+    with open(filename, "r") as f:
         for line in f:
             split = line.split()
             if len(split) > 2:
-- 
2.21.1 (Apple Git-122.3)

